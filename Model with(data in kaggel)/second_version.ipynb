{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### The Python notebook with the Data Cleaning, Model initailization, & Model fine-tuning code","metadata":{}},{"cell_type":"markdown","source":"### - Install Needed Libraries","metadata":{}},{"cell_type":"code","source":"! pip install -U accelerate --quiet\n! pip install --upgrade accelerate\n! pip install -U git+https://github.com/huggingface/transformers.git\n! pip install -U git+https://github.com/huggingface/accelerate.git\n! git clone https://github.com/aub-mind/arabert.git\n! pip install evaluate\n! pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-07-07T19:58:43.198147Z","iopub.execute_input":"2023-07-07T19:58:43.198839Z","iopub.status.idle":"2023-07-07T19:58:54.188289Z","shell.execute_reply.started":"2023-07-07T19:58:43.198803Z","shell.execute_reply":"2023-07-07T19:58:54.186840Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/utils/_process_posix.py:148\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[1;32m    149\u001b[0m flush \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:205\u001b[0m, in \u001b[0;36mspawn.__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_poll \u001b[38;5;241m=\u001b[39m use_poll\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:303\u001b[0m, in \u001b[0;36mspawn._spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m    301\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptyproc\u001b[38;5;241m.\u001b[39mpid\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pexpect/pty_spawn.py:315\u001b[0m, in \u001b[0;36mspawn._spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPtyProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ptyprocess/ptyprocess.py:315\u001b[0m, in \u001b[0;36mPtyProcess.spawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[0m\n\u001b[1;32m    314\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_write)\n\u001b[0;32m--> 315\u001b[0m exec_err_data \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m os\u001b[38;5;241m.\u001b[39mclose(exec_err_pipe_read)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install -U accelerate --quiet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m pip install --upgrade accelerate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install -U git+https://github.com/huggingface/transformers.git\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install -U git+https://github.com/huggingface/accelerate.git\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:649\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241m.\u001b[39msendline(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'child' referenced before assignment","output_type":"error"}]},{"cell_type":"markdown","source":"### Import Needed libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport evaluate\nfrom evaluate import logging\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import TextDataset,DataCollatorForLanguageModeling\nfrom transformers import GPT2TokenizerFast, pipeline\nfrom transformers import  AutoModel, utils\n# from transformers import BertTokenizer,BertForQuestionAnswering\nfrom transformers import  Trainer, TrainingArguments,AutoModelWithLMHead, GPT2LMHeadModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### - Read Data ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fatawa/88kData.csv')\ndf.head(10)\n# No. of records\nlen(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### - Cleaning data","metadata":{}},{"cell_type":"code","source":"#remove ID column\ndf = df.drop('ID', axis=1)\n# remove Null value\ndf = df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NewList = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# append unneeded fatwa\n\nNewList+=list(df[df['ans'].str.contains('انظر إلى الفتوى')].index)\nNewList+=list(df[df['ans'].str.contains('رقم:')].index)\nNewList+=list(df[df['ans'].str.contains('فتوى رقم')].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unneeded fatwa\ndf = df.drop(index=NewList, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['ans'].str.contains('นู')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(index=84488, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No. of new records\nlen(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_arabic_and_english_numerics(text):\n    # remove english letter\n    arabic_pattern = r'[^a-zA-Z\\s0-9]+'\n    numeric_pattern = r'\\d+'\n    combined_pattern = f'({arabic_pattern}|{numeric_pattern})'\n    \n    matches = re.findall(combined_pattern, text)    \n    extracted_text = ' '.join(matches)\n    \n    return extracted_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the function on the ['ans'] column\ndf['ans'] = df['ans'].apply(extract_arabic_and_english_numerics)\ndf['ans']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the function on the ['ques'] column\ndf['ques'] = df['ques'].apply(extract_arabic_and_english_numerics)\ndf['ques']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the function on the ['title'] column\ndf['title'] = df['title'].apply(extract_arabic_and_english_numerics)\ndf['title']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newlist2 = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize the answers with length less than 118 charachter\nfor k in range(len(df)):\n    \n    if len(df['ans'].iloc[k]) < 118:\n        print(df['ans'].iloc[k])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping every row that contains an answer with length less than 118 charachter\nfor k in range(len(df)):\n    \n    if len(df['ans'].iloc[k]) < 118:\n        newlist2.append(k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove from data\nnewlist2 = list(df.iloc[newlist2].index)\ndf = df.drop(index=newlist2, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Making all the dataset columns as a string, and dropping duplicates from the dataset.\ndf = df.astype(str)\ndf.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# resize data to 20000\ndf = df[0:20000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save the cleaned version of the data to the Machine as .csv file\ndf.to_csv(\"Finalized_Data2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"Finalized_Data2.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### - Model Intialization & Data Adjustement for Model Fine-Tuning","metadata":{}},{"cell_type":"code","source":"df[\"full_text\"]=\" question: \" + df[\"title\"]+df[\"ques\"]+\" answer: \"+df[\"ans\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"full_text\"][1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data to train and test\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df[\"full_text\"],test_size=0.10, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save train and test data \n\ntrain.to_csv(r'train_dataset1.txt', header=None, index=None, sep=' ', mode='a')\ntest.to_csv(r'test_dataset1.txt', header=None, index=None, sep=' ', mode='a')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrain aragpt2 model","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nimport numpy as np\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n\n    # Set pad token\n    if tokenizer.pad_token_id is not None:\n        labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    else:\n        labels_ids[labels_ids == -100] = 0\n\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    bleu_score = round(corpus_bleu(pred_str, [label_str]).score, 4)\n\n    return {\"bleu\": bleu_score}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelWithLMHead.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer.add_tokens(['<question>', '<answer>'])\ntrain_path = 'train_dataset1.txt'\ntest_path = 'test_dataset1.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.max_length=100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### - (ARAGPT2) Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def load_dataset(train_path, test_path, tokenizer):\n    train_dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=train_path,\n        block_size=128\n    )\n\n    test_dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=test_path,\n        block_size=128\n    )\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False\n    )\n    \n    eval_data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False\n    )\n\n    return train_dataset, test_dataset, data_collator, eval_data_collator\n\ntrain_dataset, test_dataset, data_collator, eval_data_collator = load_dataset(train_path, test_path, tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_data_collator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import  Seq2SeqTrainingArguments\nfrom transformers import Seq2SeqTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set training arguments - these params are not really tuned, feel free to change\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./bert2gpt-Training\",\n    evaluation_strategy=\"epoch\",\n    save_strategy = \"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    num_train_epochs=2,\n    do_train=True,\n    do_eval=True,\n    fp16=True,\n    overwrite_output_dir=True,\n    learning_rate = 1e-5,\n    weight_decay=0.01,\n    warmup_ratio = 0.05,\n    seed = 1995,\n    save_total_limit = 2,\n    load_best_model_at_end = True,\n    #push_to_hub=True,\n    optim = \"adamw_hf\",\n)\n#\n\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    data_collator = data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset\n\n)\n#    save_total_limit=3,\n#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainer.state.log_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# حساب خسارة التدريب وخسارة التحقق\ntrain_loss = np.mean(trainer.state.log_history[1][\"train_loss\"])\nvalidation_loss = np.mean(trainer.state.log_history[0][\"eval_loss\"])\n\n# عرض النتائج\nprint(\"Training Loss:\", train_loss)\nprint(\"Validation Loss:\", validation_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_loss_values = [log[\"train_loss\"] for log in trainer.state.log_history[1:]]\nvalidation_loss_values = [log[\"eval_loss\"] for log in trainer.state.log_history[0:]]\n\nepochs = range(1, len(train_loss_values) + 1)\n\nplt.plot(epochs, train_loss_values, label='Train Loss')\nplt.plot(epochs, validation_loss_values, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"aubmindlab/aragpt2-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# حساب المقاييس على مجموعة الاختبار\neval_results = trainer.evaluate()\n\n# عرض نتائج المقاييس\nprint(\"Meteor Score:\", eval_results['eval_meteor_score'])\nprint(\"ROUGE2 Precision:\", eval_results['eval_rouge2_precision'])\nprint(\"ROUGE2 Recall:\", eval_results['eval_rouge2_recall'])\nprint(\"ROUGE2 F-Measure:\", eval_results['eval_rouge2_f_measure'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import  pipeline\n\ngeneration_pipeline = pipeline('text-generation',model=\"/kaggle/input/islamweb\", tokenizer='aubmindlab/aragpt2-base')\n\n\ndef prediction(str):\n\n    str=generation_pipeline(str,max_length=300)[0]['generated_text']\n    start_index = str.index('answer:')\n    if 'والله أعلم' in str:\n        stop_index = str.index('والله أعلم')\n        str = str[start_index+8:stop_index+11].strip()\n        \n    else:\n        str = str[start_index+8:].strip()\n\n    return str\n\ndemo = gr.Interface(fn=prediction,outputs=gr.Textbox(label=\"الاجابه\"),inputs=gr.Textbox(label=\"ما هو سؤالك؟\"),title=\"نَوَّرْ\",css=\"body {background-color: green,}\")\n\ndemo.launch(share=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()\ntrainer.save_state()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"//# لحد هنا الموديل ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}